{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":115439,"databundleVersionId":13800781,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#cell-1\n!pip install wandb --quiet\nimport wandb\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nwandb_api = user_secrets.get_secret(\"WANDB_API_KEY\")\n\nwandb.login(key=wandb_api)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T02:52:34.209702Z","iopub.execute_input":"2025-11-30T02:52:34.209946Z","iopub.status.idle":"2025-11-30T02:52:47.081050Z","shell.execute_reply.started":"2025-11-30T02:52:34.209921Z","shell.execute_reply":"2025-11-30T02:52:47.080396Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m22f3000982\u001b[0m (\u001b[33m22f3000982-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"wandb.init(\n    project=\"2025-sep-dl-genai-project\",   \n    name=\"roberta_multilabel_v3\",          \n    config={\n        \"model\": \"roberta-base\",\n        \"epochs\": 8,\n        \"batch_size\": 16,\n        \"learning_rate\": 2e-5,\n        \"weight_decay\": 0.02,\n        \"warmup_ratio\": 0.1,\n        \"early_stop_patience\": 2,\n    }\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T02:53:16.481269Z","iopub.execute_input":"2025-11-30T02:53:16.481571Z","iopub.status.idle":"2025-11-30T02:53:23.355254Z","shell.execute_reply.started":"2025-11-30T02:53:16.481539Z","shell.execute_reply":"2025-11-30T02:53:23.354709Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.20.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20251130_025316-30gzokq3</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/22f3000982-indian-institute-of-technology-madras/2025-sep-dl-genai-project/runs/30gzokq3' target=\"_blank\">roberta_multilabel_v3</a></strong> to <a href='https://wandb.ai/22f3000982-indian-institute-of-technology-madras/2025-sep-dl-genai-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/22f3000982-indian-institute-of-technology-madras/2025-sep-dl-genai-project' target=\"_blank\">https://wandb.ai/22f3000982-indian-institute-of-technology-madras/2025-sep-dl-genai-project</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/22f3000982-indian-institute-of-technology-madras/2025-sep-dl-genai-project/runs/30gzokq3' target=\"_blank\">https://wandb.ai/22f3000982-indian-institute-of-technology-madras/2025-sep-dl-genai-project/runs/30gzokq3</a>"},"metadata":{}},{"execution_count":2,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/22f3000982-indian-institute-of-technology-madras/2025-sep-dl-genai-project/runs/30gzokq3?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7f90b00ceb10>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T02:53:36.292575Z","iopub.execute_input":"2025-11-30T02:53:36.293277Z","iopub.status.idle":"2025-11-30T02:53:40.722655Z","shell.execute_reply.started":"2025-11-30T02:53:36.293252Z","shell.execute_reply":"2025-11-30T02:53:40.722007Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!pip install -q transformers datasets torch scikit-learn accelerate\n\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    Trainer,\n    TrainingArguments\n)\nfrom datasets import Dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T02:53:56.290802Z","iopub.execute_input":"2025-11-30T02:53:56.291478Z","iopub.status.idle":"2025-11-30T02:55:44.241031Z","shell.execute_reply.started":"2025-11-30T02:53:56.291452Z","shell.execute_reply":"2025-11-30T02:55:44.240267Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"2025-11-30 02:55:30.116531: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764471330.295387      38 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764471330.350134      38 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"\ndevice_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\nprint(\"device\", device_name)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T02:55:44.242506Z","iopub.execute_input":"2025-11-30T02:55:44.243250Z","iopub.status.idle":"2025-11-30T02:55:44.251248Z","shell.execute_reply.started":"2025-11-30T02:55:44.243182Z","shell.execute_reply":"2025-11-30T02:55:44.250582Z"}},"outputs":[{"name":"stdout","text":"device Tesla T4\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/2025-sep-dl-gen-ai-project/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/2025-sep-dl-gen-ai-project/test.csv\")\n\nprint(train_df.head())\nprint(\"Train shape:\", train_df.shape)\nprint(\"Test shape:\", test_df.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T04:01:29.980164Z","iopub.execute_input":"2025-11-30T04:01:29.980516Z","iopub.status.idle":"2025-11-30T04:01:30.009628Z","shell.execute_reply.started":"2025-11-30T04:01:29.980490Z","shell.execute_reply":"2025-11-30T04:01:30.008745Z"}},"outputs":[{"name":"stdout","text":"   id                                               text  anger  fear  joy  \\\n0   0  the dentist that did the work apparently did a...      1     0    0   \n1   1  i'm gonna absolutely ~~suck~~ be terrible duri...      0     1    0   \n2   2  bridge: so leave me drowning calling houston, ...      0     1    0   \n3   3  after that mess i went to see my now ex-girlfr...      1     1    0   \n4   4  as he stumbled i ran off, afraid it might some...      0     1    0   \n\n   sadness  surprise                    emotions  \n0        1         0         ['anger' 'sadness']  \n1        1         0          ['fear' 'sadness']  \n2        1         0          ['fear' 'sadness']  \n3        1         0  ['anger' 'fear' 'sadness']  \n4        0         0                    ['fear']  \nTrain shape: (6827, 8)\nTest shape: (1707, 2)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import pandas as pd\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom collections import Counter\n\nnltk.download('stopwords')\n\ntrain_df = pd.read_csv(\"/kaggle/input/2025-sep-dl-gen-ai-project/train.csv\")\n\ntrain_df['clean_text'] = train_df['text'].str.lower()\n\n\nbefore_count = train_df['clean_text'].str.len().sum()\n\n\ntranslator = str.maketrans('', '', string.punctuation)\ntrain_df['clean_text'] = train_df['clean_text'].apply(lambda x: x.translate(translator))\n\nafter_count = train_df['clean_text'].str.len().sum()\n\n\nreduction_percent = ((before_count - after_count) / before_count) * 100\nprint(f\" Percentage reduction in character count: {reduction_percent:.2f}%\")\n\n\nwords = []\nfor text in train_df['clean_text']:\n    words.extend(text.split())\n\nunique_words = set(words)\nprint(\"Total unique words:\", len(unique_words))\n\nstop_words = set(stopwords.words('english'))\ncommon_stopwords = unique_words.intersection(stop_words)\n\nstopword_percent = (len(common_stopwords) / len(unique_words)) * 100\nprint(f\" Percentage of unique words that are stop words: {stopword_percent:.2f}%\")\n\nfiltered_words = [w for w in words if w not in stop_words]\nword_freq = Counter(filtered_words)\nmost_common_words = word_freq.most_common(10)\n\nprint(\"\\nTop 10 most frequent non_stop word:\")\nfor i, (word, count) in enumerate(most_common_words, 1):\n    print(f\"{i}. {word} - {count}\")\n\nfifth_most_freq_word = most_common_words[4][0]\nprint(f\"\\n 5th most frequent word (excluding stop words): '{fifth_most_freq_word}'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T04:01:32.830709Z","iopub.execute_input":"2025-11-30T04:01:32.831373Z","iopub.status.idle":"2025-11-30T04:01:32.966231Z","shell.execute_reply.started":"2025-11-30T04:01:32.831348Z","shell.execute_reply":"2025-11-30T04:01:32.965433Z"}},"outputs":[{"name":"stdout","text":" Percentage reduction in character count: 3.26%\nTotal unique words: 8365\n Percentage of unique words that are stop words: 1.54%\n\nTop 10 most frequent non_stop word:\n1. head - 539\n2. eyes - 438\n3. like - 394\n4. back - 365\n5. heart - 334\n6. one - 323\n7. face - 293\n8. get - 291\n9. time - 271\n10. still - 271\n\n 5th most frequent word (excluding stop words): 'heart'\n","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Dataset\n\nlabels = ['anger', 'fear', 'joy', 'sadness', 'surprise']\n\nprint(\"Initial rows:\", train_df.shape[0])\nprint(\"Missing text rows in train:\", train_df['text'].isnull().sum())\ntrain_df = train_df.dropna(subset=['text']).reset_index(drop=True)\nprint(\"Rows after dropping null-text:\", train_df.shape[0])\n\nfor l in labels:\n    train_df[l] = train_df[l].astype(int)\n\nif 'emotions' in train_df.columns:\n    train_df = train_df.drop(columns=['emotions'])\n\nprint(\"\\nLabel counts (whole train):\")\nprint(train_df[labels].sum())\n\nn_labels = train_df[labels].sum(axis=1)\nprint(\"\\nDistribution of #labels per sample (whole train):\")\nprint(n_labels.value_counts().sort_index())\n\ntrain_data, val_data = train_test_split(train_df, test_size=0.10, random_state=42, shuffle=True)\n\nprint(f\"\\nTrain rows: {len(train_data)}    |    Val rows: {len(val_data)}\")\n\nprint(\"\\nLabel counts (train split):\")\nprint(train_data[labels].sum())\nprint(\"\\nLabel counts (val split):\")\nprint(val_data[labels].sum())\n\ntrain_dataset = Dataset.from_pandas(train_data.reset_index(drop=True))\nval_dataset = Dataset.from_pandas(val_data.reset_index(drop=True))\n\nprint(\"\\nExample training sample (dict):\")\nprint(train_dataset[0])\n\nprint(\"\\nTest missing text rows:\", test_df['text'].isnull().sum())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T04:01:37.622457Z","iopub.execute_input":"2025-11-30T04:01:37.623090Z","iopub.status.idle":"2025-11-30T04:01:37.678337Z","shell.execute_reply.started":"2025-11-30T04:01:37.623065Z","shell.execute_reply":"2025-11-30T04:01:37.677511Z"}},"outputs":[{"name":"stdout","text":"Initial rows: 6827\nMissing text rows in train: 0\nRows after dropping null-text: 6827\n\nLabel counts (whole train):\nanger        808\nfear        3860\njoy         1660\nsadness     2171\nsurprise    1999\ndtype: int64\n\nDistribution of #labels per sample (whole train):\n0     676\n1    2743\n2    2587\n3     706\n4     112\n5       3\nName: count, dtype: int64\n\nTrain rows: 6144    |    Val rows: 683\n\nLabel counts (train split):\nanger        728\nfear        3468\njoy         1497\nsadness     1961\nsurprise    1798\ndtype: int64\n\nLabel counts (val split):\nanger        80\nfear        392\njoy         163\nsadness     210\nsurprise    201\ndtype: int64\n\nExample training sample (dict):\n{'id': 2276, 'text': 'it leaves both my hands free to be able to type.', 'anger': 0, 'fear': 0, 'joy': 1, 'sadness': 0, 'surprise': 0, 'clean_text': 'it leaves both my hands free to be able to type'}\n\nTest missing text rows: 0\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nMODEL_NAME = \"roberta-base\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\ndef tokenize_function(example):\n    return tokenizer(\n        example[\"text\"],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=256\n    )\n\ntrain_tokenized = train_dataset.map(tokenize_function, batched=True)\nval_tokenized = val_dataset.map(tokenize_function, batched=True)\n\nlabel_cols = ['anger', 'fear', 'joy', 'sadness', 'surprise']\n\ntrain_tokenized = train_tokenized.rename_column(\"anger\", \"labels_anger\")\ntrain_tokenized = train_tokenized.rename_column(\"fear\", \"labels_fear\")\ntrain_tokenized = train_tokenized.rename_column(\"joy\", \"labels_joy\")\ntrain_tokenized = train_tokenized.rename_column(\"sadness\", \"labels_sadness\")\ntrain_tokenized = train_tokenized.rename_column(\"surprise\", \"labels_surprise\")\n\nval_tokenized = val_tokenized.rename_column(\"anger\", \"labels_anger\")\nval_tokenized = val_tokenized.rename_column(\"fear\", \"labels_fear\")\nval_tokenized = val_tokenized.rename_column(\"joy\", \"labels_joy\")\nval_tokenized = val_tokenized.rename_column(\"sadness\", \"labels_sadness\")\nval_tokenized = val_tokenized.rename_column(\"surprise\", \"labels_surprise\")\n\ntrain_tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"] + [f\"labels_{l}\" for l in label_cols])\nval_tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"] + [f\"labels_{l}\" for l in label_cols])\n\nprint(\"Tokenization complete!\")\nprint(\"Train tokenized sample:\")\nprint(train_tokenized[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T04:01:46.993724Z","iopub.execute_input":"2025-11-30T04:01:46.994367Z","iopub.status.idle":"2025-11-30T04:01:48.349386Z","shell.execute_reply.started":"2025-11-30T04:01:46.994343Z","shell.execute_reply":"2025-11-30T04:01:48.348596Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6144 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29daab9c1ac14c19b3868f7969e5614d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/683 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ad31b0d240b4d6eb9c43d30583c93bc"}},"metadata":{}},{"name":"stdout","text":"Tokenization complete!\nTrain tokenized sample:\n{'labels_anger': tensor(0), 'labels_fear': tensor(0), 'labels_joy': tensor(1), 'labels_sadness': tensor(0), 'labels_surprise': tensor(0), 'input_ids': tensor([   0,  405, 3607,  258,  127, 1420,  481,    7,   28,  441,    7, 1907,\n           4,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom transformers import AutoModel\n\nclass RobertaMultiLabel(nn.Module):\n    def __init__(self, model_name, num_labels=5):\n        super(RobertaMultiLabel, self).__init__()\n        self.roberta = AutoModel.from_pretrained(model_name)\n        self.dropout = nn.Dropout(0.3)\n        self.classifier = nn.Linear(self.roberta.config.hidden_size, num_labels)\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs[0][:, 0]  # CLS token output\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        loss = None\n        if labels is not None:\n            pos_weights = torch.tensor([1.2, 0.9, 1.1, 1.0, 1.3], device=logits.device)\n            loss_fct = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n            loss = loss_fct(logits, labels.float())\n\n        return {\"loss\": loss, \"logits\": logits}\n\n\ndef collate_fn(batch):\n    \"\"\"\n    Custom collate function to combine multiple emotion label columns into one tensor.\n    \"\"\"\n    input_ids = torch.stack([torch.tensor(x[\"input_ids\"]).clone().detach() for x in batch])\n    attention_mask = torch.stack([torch.tensor(x[\"attention_mask\"]).clone().detach() for x in batch])\n\n    labels = torch.stack([\n        torch.tensor([\n            x[\"labels_anger\"],\n            x[\"labels_fear\"],\n            x[\"labels_joy\"],\n            x[\"labels_sadness\"],\n            x[\"labels_surprise\"]\n        ]).clone().detach().float()\n        for x in batch\n    ])\n\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"labels\": labels\n    }\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T04:04:43.790539Z","iopub.execute_input":"2025-11-30T04:04:43.790838Z","iopub.status.idle":"2025-11-30T04:04:43.800370Z","shell.execute_reply.started":"2025-11-30T04:04:43.790817Z","shell.execute_reply":"2025-11-30T04:04:43.799426Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"print(train_tokenized[0].keys())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T04:04:45.979665Z","iopub.execute_input":"2025-11-30T04:04:45.980050Z","iopub.status.idle":"2025-11-30T04:04:45.986852Z","shell.execute_reply.started":"2025-11-30T04:04:45.980028Z","shell.execute_reply":"2025-11-30T04:04:45.986098Z"}},"outputs":[{"name":"stdout","text":"dict_keys(['labels_anger', 'labels_fear', 'labels_joy', 'labels_sadness', 'labels_surprise', 'input_ids', 'attention_mask'])\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"#cell-6\nwandb.init(\n    project=\"2025-sep-dl-genai-project\",\n    name=\"roberta_multilabel_stable\",\n    config={\n        \"model\": \"roberta-base\",\n        \"epochs\": 8,\n        \"batch_size\": 16,\n        \"learning_rate\": 2e-5,\n        \"weight_decay\": 0.02,\n        \"warmup_ratio\": 0.1,\n        \"early_stop_patience\": 2\n    }\n)\n\n# ,,--------------- Model----------------------------\nclass RobertaMultiLabel(nn.Module):\n    def __init__(self, model_name, num_labels=5):\n        super().__init__()\n        from transformers import AutoModel\n        self.roberta = AutoModel.from_pretrained(model_name)\n        self.dropout = nn.Dropout(0.2)      # dropped from 0.4 → 0.2 (better)\n        self.classifier = nn.Linear(self.roberta.config.hidden_size, num_labels)\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.last_hidden_state[:, 0]\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        return {\"logits\": logits}\n\n\ndef collate_fn(batch):\n    input_ids = torch.stack([x[\"input_ids\"] for x in batch])\n    attention_mask = torch.stack([x[\"attention_mask\"] for x in batch])\n    labels = torch.stack([\n        torch.stack([\n            x[\"labels_anger\"], x[\"labels_fear\"], x[\"labels_joy\"],\n            x[\"labels_sadness\"], x[\"labels_surprise\"]\n        ])\n        for x in batch\n    ])\n    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n\n\ndef evaluate_model(model, dataloader, loss_fn, thresholds):\n    model.eval()\n    total_loss = 0.0\n    all_logits, all_labels = [], []\n\n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"labels\"].to(device).float()\n\n            logits = model(input_ids=input_ids, attention_mask=attention_mask)[\"logits\"]\n            loss = loss_fn(logits, labels)\n            total_loss += loss.item()\n\n            all_logits.append(torch.sigmoid(logits).cpu().numpy())\n            all_labels.append(labels.cpu().numpy())\n\n    avg_loss = total_loss / len(dataloader)\n\n    all_logits = np.vstack(all_logits)\n    all_labels = np.vstack(all_labels)\n\n    thresholds = np.array(thresholds)\n    preds = (all_logits >= thresholds).astype(int)\n\n    f1_macro = f1_score(all_labels, preds, average=\"macro\")\n    f1_micro = f1_score(all_labels, preds, average=\"micro\")\n    acc = accuracy_score(all_labels, preds)\n\n    try:\n        roc = roc_auc_score(all_labels, all_logits, average=\"macro\")\n    except:\n        roc = 0.0\n\n    return {\n        \"loss\": avg_loss,\n        \"f1_macro\": f1_macro,\n        \"f1_micro\": f1_micro,\n        \"accuracy\": acc,\n        \"roc_auc\": roc\n    }\n\n\n\nMODEL_NAME = \"roberta-base\"\nnum_epochs = 8\nper_device_train_batch_size = 16\nper_device_eval_batch_size = 16\ngrad_accum_steps = 2\nlearning_rate = 2e-5\nweight_decay = 0.02\nwarmup_ratio = 0.1\nmax_grad_norm = 1.0\nearly_stop_patience = 2\n\n\n\ntrain_loader = DataLoader(\n    train_tokenized,\n    batch_size=per_device_train_batch_size,\n    shuffle=True,\n    collate_fn=collate_fn\n)\nval_loader = DataLoader(\n    val_tokenized,\n    batch_size=per_device_eval_batch_size,\n    shuffle=False,\n    collate_fn=collate_fn\n)\n\nmodel = RobertaMultiLabel(MODEL_NAME, num_labels=5).to(device)\noptimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n\nnum_training_steps = len(train_loader) // grad_accum_steps * num_epochs\nscheduler = get_cosine_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=int(num_training_steps * warmup_ratio),\n    num_training_steps=num_training_steps\n)\n\n# ===== POS WEIGHTS FROM CELL-A =====\npos_weights = torch.tensor(pos_weights_list, device=device)\nloss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n\nprint(\"Using pos_weights:\", pos_weights)\nprint(\"Using thresholds:\", per_label_thresholds)\n\n\n# ===== TRAIN LOOP =====\nbest_f1 = 0\nepochs_no_improve = 0\nglobal_step = 0\n\nfor epoch in range(1, num_epochs + 1):\n    model.train()\n    running_loss = 0.0\n    start_time = time.time()\n    optimizer.zero_grad()\n\n    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch}/{num_epochs}\")\n    for step, batch in pbar:\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device).float()\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs[\"logits\"]\n        loss = loss_fn(logits, labels) / grad_accum_steps\n        loss.backward()\n        running_loss += loss.item() * grad_accum_steps\n\n        if (step + 1) % grad_accum_steps == 0:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n            global_step += 1\n\n        pbar.set_postfix({\n            \"loss\": f\"{running_loss/(step+1):.4f}\",\n            \"lr\": scheduler.get_last_lr()[0]\n        })\n\n    # ---- UPDATED EVAL CALL ----\n    val_metrics = evaluate_model(\n        model,\n        val_loader,\n        loss_fn,\n        thresholds=per_label_thresholds\n    )\n\n    elapsed = time.time() - start_time\n\n    print(f\"\\nEpoch {epoch}/{num_epochs} — time: {elapsed/60:.2f} min\")\n    print(f\"  Train loss: {running_loss/len(train_loader):.4f}\")\n    print(f\"  Val F1-macro: {val_metrics['f1_macro']:.4f} | F1-micro: {val_metrics['f1_micro']:.4f} | ROC-AUC: {val_metrics['roc_auc']:.4f}\")\n\n    wandb.log({\n        \"epoch\": epoch,\n        \"train_loss\": running_loss / len(train_loader),\n        \"val_loss\": val_metrics[\"loss\"],\n        \"f1_macro\": val_metrics[\"f1_macro\"],\n        \"f1_micro\": val_metrics[\"f1_micro\"],\n        \"roc_auc\": val_metrics[\"roc_auc\"],\n        \"accuracy\": val_metrics[\"accuracy\"],\n        \"learning_rate\": scheduler.get_last_lr()[0]\n    })\n\n    if val_metrics[\"f1_macro\"] > best_f1:\n        best_f1 = val_metrics[\"f1_macro\"]\n        epochs_no_improve = 0\n        best_dir = \"./roberta_emotion_best\"\n        os.makedirs(best_dir, exist_ok=True)\n        torch.save(model.state_dict(), os.path.join(best_dir, \"pytorch_model.bin\"))\n        wandb.save(os.path.join(best_dir, \"pytorch_model.bin\"))\n        print(f\" New best model saved! F1-macro={best_f1:.4f}\")\n    else:\n        epochs_no_improve += 1\n        print(f\"  No improvement for {epochs_no_improve} epoch(s).\")\n\n    if epochs_no_improve >= early_stop_patience:\n        print(\"Early stopping triggered.\")\n        break\n\nprint(f\"\\nTraining complete. Best F1-macro: {best_f1:.4f}\")\n\nwandb.finish()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T04:53:43.248565Z","iopub.execute_input":"2025-11-30T04:53:43.249189Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#cell-8\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom datasets import Dataset\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Tokenize test data \ntest_dataset = Dataset.from_pandas(test_df)\ntest_tokenized = test_dataset.map(\n    lambda x: tokenizer(x[\"text\"], truncation=True, padding=\"max_length\", max_length=256),\n    batched=True\n)\ntest_tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n\n\nbest_model_path = \"./roberta_emotion_best/pytorch_model.bin\"\nmodel.load_state_dict(torch.load(best_model_path, map_location=device))\nmodel.to(device)\nmodel.eval()\nTHRESHOLD = 0.5   \n\nprint(\"Using global threshold:\", THRESHOLD)\n\npreds = []\ntest_loader = torch.utils.data.DataLoader(test_tokenized, batch_size=16, shuffle=False)\n\nfor batch in tqdm(test_loader, desc=\"Predicting\"):\n    with torch.no_grad():\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs[\"logits\"]\n        preds.append(torch.sigmoid(logits).cpu().numpy())\n\npreds = np.vstack(preds)  \n\n# Convert to binary labels using global threshold\nbinary_preds = (preds >= THRESHOLD).astype(int)\n\nsubmission = pd.DataFrame(\n    binary_preds,\n    columns=[\"anger\", \"fear\", \"joy\", \"sadness\", \"surprise\"]\n)\nsubmission.insert(0, \"id\", test_df[\"id\"])\n\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"submission.csv saved successfully!\")\nsubmission.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T04:49:45.989293Z","iopub.execute_input":"2025-11-30T04:49:45.989559Z","iopub.status.idle":"2025-11-30T04:50:07.901977Z","shell.execute_reply.started":"2025-11-30T04:49:45.989538Z","shell.execute_reply":"2025-11-30T04:50:07.901269Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1707 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adb01954c3174c9db0adfefee6518050"}},"metadata":{}},{"name":"stdout","text":"Using global threshold: 0.5\n","output_type":"stream"},{"name":"stderr","text":"Predicting: 100%|██████████| 107/107 [00:21<00:00,  5.03it/s]","output_type":"stream"},{"name":"stdout","text":"submission.csv saved successfully!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"   id  anger  fear  joy  sadness  surprise\n0   0      1     1    0        0         0\n1   1      0     0    0        0         0\n2   2      1     1    0        0         1\n3   3      0     1    0        0         0\n4   4      0     1    0        0         1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>anger</th>\n      <th>fear</th>\n      <th>joy</th>\n      <th>sadness</th>\n      <th>surprise</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"\n# import os\n\n# submission_path = \"/kaggle/working/submission.csv\"\n# submission.to_csv(submission_path, index=False)\n\n# print(f\"submission.csv saved at: {submission_path}\")\n# print(\"Files in working dir:\", os.listdir(\"/kaggle/working\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T13:10:18.374553Z","iopub.execute_input":"2025-11-29T13:10:18.374841Z","iopub.status.idle":"2025-11-29T13:10:18.384043Z","shell.execute_reply.started":"2025-11-29T13:10:18.374817Z","shell.execute_reply":"2025-11-29T13:10:18.383328Z"}},"outputs":[{"name":"stdout","text":"submission.csv saved at: /kaggle/working/submission.csv\nFiles in working dir: ['wandb', '.virtual_documents', 'submission.csv', 'hf_model', 'roberta_emotion_best']\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# Milestone 3 below ","metadata":{}},{"cell_type":"code","source":"# import math\n\n# # Q1: Sigmoid output for z = 1.5\n# z = 1.5\n# sigmoid = 1 / (1 + math.exp(-z))\n# print(\"1️ Sigmoid(1.5):\", round(sigmoid, 3))  # 0.818\n\n# # Q2: Total parameters in BERT-BASE\n# bert_base_params = 110_000_000\n# print(\"2️ BERT-BASE total parameters:\", bert_base_params, \"(~110M)\")\n\n# # Q3: Binary Cross Entropy Loss (y=1, p=0.9)\n# y, p = 1, 0.9\n# bce_loss = - (y * math.log(p) + (1 - y) * math.log(1 - p))\n# print(\"3️ Binary Cross-Entropy Loss:\", round(bce_loss, 3))  # 0.105\n\n# # Q4: Special tokens added by BERT for single sequence\n# special_tokens = 2  # [CLS], [SEP]\n# print(\"4️ Special tokens added:\", special_tokens)\n\n# # Q5: Number of encoder layers in BERT-BASE\n# encoder_layers = 12\n# print(\"5️ Encoder layers in BERT-BASE:\", encoder_layers)\n\n# # Q6: Recommended Adam variant for fine-tuning Transformers\n# adam_variant = \"AdamW\"\n# print(\"6️ Recommended Adam variant:\", adam_variant)\n\n# # Q7: Nonlinearity used in Transformer blocks\n# activation_fn = \"GELU\"\n# print(\"7️ Activation function in Transformer blocks:\", activation_fn)\n\n# # Q8: Optimal learning rate range for fine-tuning Transformers\n# lr_range = \"2e-5 to 5e-5\"\n# print(\"8️ Typical fine-tuning learning rate range:\", lr_range)\n\n# # Q9: Dropout rate in BERT classification head (default for BERT-base)\n# dropout_rate = 0.1\n# print(\"9️ Dropout rate in BERT classification head:\", dropout_rate)\n\n# # Q10: Practical upper limit for batch size on Colab T4 GPU (16GB VRAM)\n# # For BERT-base (sequence length 128-256)\n# batch_size_limit = 32\n# print(\" Practical upper batch size limit (Colab T4):\", batch_size_limit)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Milestone 4 below","metadata":{}},{"cell_type":"code","source":"# import re\n# import pandas as pd\n# from collections import Counter\n\n# train_path = \"/kaggle/input/2025-sep-dl-gen-ai-project/train.csv\"\n# df = pd.read_csv(train_path)\n\n# def tokenize(x):\n#     return re.findall(r\"[A-Za-z0-9']+\", str(x).lower())\n\n# counter = Counter()\n# for t in df[\"text\"].fillna(\"\"):\n#     counter.update(tokenize(t))\n\n# # remove rare words (min_freq = 3)\n# counter = Counter({w:c for w,c in counter.items() if c >= 3})\n\n# PAD = \"<pad>\"\n# UNK = \"<unk>\"\n# itos = [PAD, UNK] + sorted(counter.keys())\n# stoi = {w:i for i,w in enumerate(itos)}\n\n# print(\"Vocabulary Size:\", len(itos))\n# print(\"Padding index:\", stoi[PAD])\n# print(\"Unknown index:\", stoi[UNK])\n\n# print(\"\\nIndices:\")\n# print(\"happy:\", stoi.get(\"happy\", None))\n# print(\"alone:\", stoi.get(\"alone\", None))\n# print(\"sad:\", stoi.get(\"sad\", None))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}