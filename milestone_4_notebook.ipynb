{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 115439,
          "databundleVersionId": 13800781,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31153,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/22f3000982/DL_genai_project/blob/main/milestone_4_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Milestone 4 ‚Äî Sequence Modeling with LSTM and GRU\n",
        "\n",
        "This milestone introduces **deep learning models (LSTM / GRU)** that are specifically designed to capture the **order and contextual relationships** between words in a sequence.\n",
        "\n",
        "---\n",
        "\n",
        "##  Suggested Readings\n",
        "- [LSTM](https://docs.pytorch.org/docs/stable/generated/torch.nn.GRU.html)\n",
        "- [GRU](https://docs.pytorch.org/docs/stable/generated/torch.nn.LSTM.html)\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Instructions\n",
        "\n",
        "Use the **constants and helper functions** provided in the next cell to answer all **Milestone-4 questions**.\n",
        "\n",
        "Perform the following tasks on the **training dataset** provided as part of the Kaggle competition:\n",
        "\n",
        "üîó **Competition Link:**  \n",
        "[2025-Sep-DL-Gen-AI-Project](https://www.kaggle.com/competitions/2025-sep-dl-gen-ai-project)\n"
      ],
      "metadata": {
        "id": "e2ogIMAMt4VL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "naJr2EGft4VN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import Counter\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T17:22:49.567144Z",
          "iopub.execute_input": "2025-10-24T17:22:49.567471Z",
          "iopub.status.idle": "2025-10-24T17:22:49.573706Z",
          "shell.execute_reply.started": "2025-10-24T17:22:49.567448Z",
          "shell.execute_reply": "2025-10-24T17:22:49.572570Z"
        },
        "id": "JbLUKraxt4VO"
      },
      "outputs": [],
      "execution_count": 14
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set seeds and Constants"
      ],
      "metadata": {
        "id": "4J6MM3M4t4VO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#----------------------------- DON'T CHANGE THIS --------------------------\n",
        "DATA_SEED = 67\n",
        "TRAINING_SEED = 1234\n",
        "MAX_LEN = 50\n",
        "BATCH_SIZE = 64\n",
        "EMB_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 5\n",
        "\n",
        "random.seed(DATA_SEED)\n",
        "np.random.seed(DATA_SEED)\n",
        "torch.manual_seed(DATA_SEED)\n",
        "torch.cuda.manual_seed(DATA_SEED)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T17:24:38.648298Z",
          "iopub.execute_input": "2025-10-24T17:24:38.648610Z",
          "iopub.status.idle": "2025-10-24T17:24:38.656492Z",
          "shell.execute_reply.started": "2025-10-24T17:24:38.648588Z",
          "shell.execute_reply": "2025-10-24T17:24:38.655516Z"
        },
        "id": "FziGMCfXt4VO"
      },
      "outputs": [],
      "execution_count": 15
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Vocab"
      ],
      "metadata": {
        "id": "WdF4Ds3-t4VP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/train (2).csv\"  # enter your data path here\n",
        "df = pd.read_csv(data_path)  # read it and store it in df"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T16:37:54.421622Z",
          "iopub.execute_input": "2025-10-24T16:37:54.422106Z",
          "iopub.status.idle": "2025-10-24T16:37:54.474780Z",
          "shell.execute_reply.started": "2025-10-24T16:37:54.422083Z",
          "shell.execute_reply": "2025-10-24T16:37:54.473840Z"
        },
        "id": "SKh8SkoYt4VP"
      },
      "outputs": [],
      "execution_count": 16
    },
    {
      "cell_type": "code",
      "source": [
        "# Split train df into train_df(80%) and test_df (20%) use seed\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=DATA_SEED)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T16:40:42.943254Z",
          "iopub.execute_input": "2025-10-24T16:40:42.944416Z",
          "iopub.status.idle": "2025-10-24T16:40:42.960903Z",
          "shell.execute_reply.started": "2025-10-24T16:40:42.944346Z",
          "shell.execute_reply": "2025-10-24T16:40:42.959758Z"
        },
        "id": "Yrv4U9a7t4VP"
      },
      "outputs": [],
      "execution_count": 17
    },
    {
      "cell_type": "code",
      "source": [
        "# create a simple space-based tokenizer.\n",
        "def tokenize(text):\n",
        "    return text.split()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T16:44:17.602830Z",
          "iopub.execute_input": "2025-10-24T16:44:17.603704Z",
          "iopub.status.idle": "2025-10-24T16:44:17.608521Z",
          "shell.execute_reply.started": "2025-10-24T16:44:17.603674Z",
          "shell.execute_reply": "2025-10-24T16:44:17.607566Z"
        },
        "id": "HJdeRv_ht4VP"
      },
      "outputs": [],
      "execution_count": 18
    },
    {
      "cell_type": "code",
      "source": [
        "# Use counter to count all tokens in train_df\n",
        "token_counter = Counter()\n",
        "for text in train_df['text']:\n",
        "    token_counter.update(tokenize(text))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T16:44:45.867518Z",
          "iopub.execute_input": "2025-10-24T16:44:45.867829Z",
          "iopub.status.idle": "2025-10-24T16:44:45.902212Z",
          "shell.execute_reply.started": "2025-10-24T16:44:45.867807Z",
          "shell.execute_reply": "2025-10-24T16:44:45.900995Z"
        },
        "id": "kxjVnGSTt4VQ"
      },
      "outputs": [],
      "execution_count": 19
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create train and val dataloaders"
      ],
      "metadata": {
        "id": "BBR5zOYPt4VQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#----------------------------- DON'T CHANGE THIS --------------------------\n",
        "specials = ['<unk>', '<pad>']\n",
        "min_freq = 2\n",
        "vocab_list = specials + [token for token, freq in token_counter.items() if freq >= min_freq]\n",
        "word2idx = {token: i for i, token in enumerate(vocab_list)}\n",
        "UNK_IDX = word2idx['<unk>']\n",
        "PAD_IDX = word2idx['<pad>']\n",
        "VOCAB_SIZE = len(vocab_list)\n",
        "\n",
        "def text_pipeline(text):\n",
        "    \"\"\"Converts text to a list of indices using the word2idx dict.\"\"\"\n",
        "    tokens = tokenize(text)\n",
        "    return [word2idx.get(token, UNK_IDX) for token in tokens]\n",
        "\n",
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.texts = dataframe['text'].values\n",
        "        self.labels = dataframe[['anger', 'fear', 'joy', 'sadness', 'surprise']].values.astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx], self.labels[idx]\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list = [], []\n",
        "    for (_text, _labels) in batch:\n",
        "        label_list.append(_labels)\n",
        "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)[:MAX_LEN]\n",
        "        text_list.append(processed_text)\n",
        "    label_list = torch.tensor(label_list, dtype=torch.float32)\n",
        "    text_list = pad_sequence(text_list, batch_first=True, padding_value=PAD_IDX)\n",
        "    if text_list.shape[1] < MAX_LEN:\n",
        "        pad_tensor = torch.full(\n",
        "            (text_list.shape[0], MAX_LEN - text_list.shape[1]),\n",
        "            PAD_IDX,\n",
        "            dtype=torch.int64\n",
        "        )\n",
        "        text_list = torch.cat((text_list, pad_tensor), dim=1)\n",
        "\n",
        "    return text_list, label_list\n",
        "\n",
        "# Create train and val dataloaders\n",
        "train_dataset = EmotionDataset(train_df)\n",
        "val_dataset = EmotionDataset(test_df)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T17:21:56.467351Z",
          "iopub.execute_input": "2025-10-24T17:21:56.467673Z",
          "iopub.status.idle": "2025-10-24T17:21:56.483356Z",
          "shell.execute_reply.started": "2025-10-24T17:21:56.467652Z",
          "shell.execute_reply": "2025-10-24T17:21:56.482391Z"
        },
        "id": "DTFAKSgPt4VQ"
      },
      "outputs": [],
      "execution_count": 21
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "1b8f55c1",
        "outputId": "67fd56ec-52f1-4fba-ee2a-b93d6882a099"
      },
      "source": [
        "# Training parameters\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "simple_lstm_model = SimpleLSTM(VOCAB_SIZE, EMB_DIM, HIDDEN_DIM, OUTPUT_DIM, PAD_IDX)\n",
        "criterion = nn.BCEWithLogitsLoss() # Use BCEWithLogitsLoss for multi-label classification\n",
        "optimizer = optim.Adam(simple_lstm_model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Function to calculate F1 score\n",
        "def calculate_f1(y_true, y_pred):\n",
        "    y_pred = torch.sigmoid(y_pred).round().detach().cpu().numpy()\n",
        "    y_true = y_true.detach().cpu().numpy()\n",
        "    return f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "# Training loop\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "simple_lstm_model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    simple_lstm_model.train()\n",
        "    running_loss = 0.0\n",
        "    running_f1 = 0.0\n",
        "\n",
        "    for text, labels in train_dataloader:\n",
        "        text, labels = text.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = simple_lstm_model(text)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * text.size(0)\n",
        "        running_f1 += calculate_f1(labels, outputs) * text.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    epoch_f1 = running_f1 / len(train_dataset)\n",
        "\n",
        "    # Evaluation loop\n",
        "    simple_lstm_model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    val_running_f1 = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for text, labels in val_dataloader:\n",
        "            text, labels = text.to(device), labels.to(device)\n",
        "            outputs = simple_lstm_model(text)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_running_loss += loss.item() * text.size(0)\n",
        "            val_running_f1 += calculate_f1(labels, outputs) * text.size(0)\n",
        "\n",
        "    val_epoch_loss = val_running_loss / len(val_dataset)\n",
        "    val_epoch_f1 = val_running_f1 / len(val_dataset)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Train F1: {epoch_f1:.4f}, Val Loss: {val_epoch_loss:.4f}, Val F1: {val_epoch_f1:.4f}\")\n",
        "\n",
        "print(\"Training finished!\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Train Loss: 0.5794, Train F1: 0.1454, Val Loss: 0.5663, Val F1: 0.1425\n",
            "Epoch 2/10, Train Loss: 0.5664, Train F1: 0.1448, Val Loss: 0.5655, Val F1: 0.1425\n",
            "Epoch 3/10, Train Loss: 0.5655, Train F1: 0.1535, Val Loss: 0.5662, Val F1: 0.1435\n",
            "Epoch 4/10, Train Loss: 0.5638, Train F1: 0.1624, Val Loss: 0.5667, Val F1: 0.1513\n",
            "Epoch 5/10, Train Loss: 0.5621, Train F1: 0.1682, Val Loss: 0.5638, Val F1: 0.1638\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3475855896.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimple_lstm_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9f1a13d"
      },
      "source": [
        "# Simple LSTM Model\n",
        "class SimpleLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, pad_idx):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        output, (hidden, cell) = self.lstm(embedded)\n",
        "        hidden = hidden.squeeze(0)\n",
        "        return self.fc(hidden)\n",
        "\n",
        "# Bidirectional LSTM Model\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, pad_idx):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        output, (hidden, cell) = self.lstm(embedded)\n",
        "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
        "        return self.fc(hidden)\n",
        "\n",
        "# Stacked GRU Model (2 layers)\n",
        "class StackedGRU(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, pad_idx, n_layers=2):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=n_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        output, hidden = self.gru(embedded)\n",
        "        hidden = hidden[-1,:,:]\n",
        "        return self.fc(hidden)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "Q0KEXh1wt4VQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. What are the vocabulary size, padding token index, and unknown token index for the above dataset?"
      ],
      "metadata": {
        "id": "nwq9_wK4t4VQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- write your code here -------------------------------\n",
        "print(f\"Vocabulary size: {VOCAB_SIZE}\")\n",
        "print(f\"Padding token index: {PAD_IDX}\")\n",
        "print(f\"Unknown token index: {UNK_IDX}\")\n",
        "#-------------------------------------------------------------------------"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T16:57:14.541326Z",
          "iopub.execute_input": "2025-10-24T16:57:14.541611Z",
          "iopub.status.idle": "2025-10-24T16:57:14.546909Z",
          "shell.execute_reply.started": "2025-10-24T16:57:14.541593Z",
          "shell.execute_reply": "2025-10-24T16:57:14.546053Z"
        },
        "id": "InMGo-zwt4VQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d4ecc52-2488-4330-a197-c6fc1f51b271"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 5730\n",
            "Padding token index: 1\n",
            "Unknown token index: 0\n"
          ]
        }
      ],
      "execution_count": 22
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2.What are the indices for the words \"happy\", \"alone\", and \"sad\" in the vocabulary?"
      ],
      "metadata": {
        "id": "W2s9owsCt4VR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- write your code here -------------------------------\n",
        "print(f\"Index for 'happy': {word2idx.get('happy', UNK_IDX)}\")\n",
        "print(f\"Index for 'alone': {word2idx.get('alone', UNK_IDX)}\")\n",
        "print(f\"Index for 'sad': {word2idx.get('sad', UNK_IDX)}\")\n",
        "#-------------------------------------------------------------------------"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T16:58:00.297702Z",
          "iopub.execute_input": "2025-10-24T16:58:00.298047Z",
          "iopub.status.idle": "2025-10-24T16:58:00.304387Z",
          "shell.execute_reply.started": "2025-10-24T16:58:00.298024Z",
          "shell.execute_reply": "2025-10-24T16:58:00.302684Z"
        },
        "id": "LVNCVQADt4VR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11b8735e-52a1-4e00-dd6f-fe7ce4abe16f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index for 'happy': 1578\n",
            "Index for 'alone': 2525\n",
            "Index for 'sad': 885\n"
          ]
        }
      ],
      "execution_count": 23
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T17:28:15.777599Z",
          "iopub.execute_input": "2025-10-24T17:28:15.777917Z",
          "iopub.status.idle": "2025-10-24T17:28:15.784144Z",
          "shell.execute_reply.started": "2025-10-24T17:28:15.777894Z",
          "shell.execute_reply": "2025-10-24T17:28:15.783195Z"
        },
        "id": "N83sTQ8At4VR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Get one batch to test shapes\n",
        "text_batch, labels_batch = next(iter(train_dataloader))\n",
        "emb_layer = nn.Embedding(VOCAB_SIZE, EMB_DIM)\n",
        "embedded_batch = emb_layer(text_batch)\n",
        "\n",
        "# Simple LSTM layer Output Shape (Use constants defined in 2nd cell)\n",
        "# lstm = # Create your lstm layer here\n",
        "#read_output = lstm(embedded_batch)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T17:43:27.729908Z",
          "iopub.execute_input": "2025-10-24T17:43:27.730227Z",
          "iopub.status.idle": "2025-10-24T17:43:27.805156Z",
          "shell.execute_reply.started": "2025-10-24T17:43:27.730207Z",
          "shell.execute_reply": "2025-10-24T17:43:27.804231Z"
        },
        "id": "_6frZYdht4VR"
      },
      "outputs": [],
      "execution_count": 24
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3. What is the output shape of the Embedding layer?\n"
      ],
      "metadata": {
        "id": "j2m_SwrZt4VR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- write your code here -------------------------------\n",
        "print(f\"Output shape of Embedding layer: {embedded_batch.shape}\")\n",
        "#-------------------------------------------------------------------------"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T17:43:30.395104Z",
          "iopub.execute_input": "2025-10-24T17:43:30.395595Z",
          "iopub.status.idle": "2025-10-24T17:43:30.401656Z",
          "shell.execute_reply.started": "2025-10-24T17:43:30.395569Z",
          "shell.execute_reply": "2025-10-24T17:43:30.400766Z"
        },
        "id": "h4C8uR4zt4VR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad373bb7-7c9e-42c4-b840-7ff564a31a15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape of Embedding layer: torch.Size([64, 50, 100])\n"
          ]
        }
      ],
      "execution_count": 25
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q4. What will be output shape of simple LSTM layer"
      ],
      "metadata": {
        "id": "cQi8Xfhit4VR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- write your code here -------------------------------\n",
        "lstm = nn.LSTM(EMB_DIM, HIDDEN_DIM, batch_first=True)\n",
        "lstm_output, (lstm_hidden, lstm_cell) = lstm(embedded_batch)\n",
        "print(f\"Output shape of simple LSTM layer: {lstm_output.shape}\")\n",
        "#-------------------------------------------------------------------------"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T17:43:33.022293Z",
          "iopub.execute_input": "2025-10-24T17:43:33.022614Z",
          "iopub.status.idle": "2025-10-24T17:43:33.027754Z",
          "shell.execute_reply.started": "2025-10-24T17:43:33.022577Z",
          "shell.execute_reply": "2025-10-24T17:43:33.026937Z"
        },
        "id": "eL299270t4VS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b0f4cc6-ba9c-4446-80ab-96b2cbec810a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape of simple LSTM layer: torch.Size([64, 50, 256])\n"
          ]
        }
      ],
      "execution_count": 26
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q5. What is the 'hidden' state shape from a simple LSTM?"
      ],
      "metadata": {
        "id": "2WyMjs-Pt4VS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- write your code here -------------------------------\n",
        "print(f\"'Hidden' state shape from a simple LSTM: {lstm_hidden.shape}\")\n",
        "#-------------------------------------------------------------------------"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T17:43:37.314103Z",
          "iopub.execute_input": "2025-10-24T17:43:37.314400Z",
          "iopub.status.idle": "2025-10-24T17:43:37.319394Z",
          "shell.execute_reply.started": "2025-10-24T17:43:37.314379Z",
          "shell.execute_reply": "2025-10-24T17:43:37.318208Z"
        },
        "id": "SFu7xJO2t4VS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "831f786d-75a8-48b2-f419-4ff1cdb22185"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Hidden' state shape from a simple LSTM: torch.Size([1, 64, 256])\n"
          ]
        }
      ],
      "execution_count": 27
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q6. What is the 'hidden' state shape from a simple GRU?"
      ],
      "metadata": {
        "id": "P-_KR3gJt4VS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# similarly do it for gru and find hidden state shape\n",
        "# ------------------- write your code here -------------------------------\n",
        "gru = nn.GRU(EMB_DIM, HIDDEN_DIM, batch_first=True)\n",
        "gru_output, gru_hidden = gru(embedded_batch)\n",
        "print(f\"'Hidden' state shape from a simple GRU: {gru_hidden.shape}\")\n",
        "#-------------------------------------------------------------------------"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T17:48:43.149832Z",
          "iopub.execute_input": "2025-10-24T17:48:43.150446Z",
          "iopub.status.idle": "2025-10-24T17:48:43.205538Z",
          "shell.execute_reply.started": "2025-10-24T17:48:43.150421Z",
          "shell.execute_reply": "2025-10-24T17:48:43.204760Z"
        },
        "id": "6zu5csBlt4VS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ad2af01-71e8-4490-f8b0-c3c89daa4c1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Hidden' state shape from a simple GRU: torch.Size([1, 64, 256])\n"
          ]
        }
      ],
      "execution_count": 28
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q7. What is the 'output' tensor shape from a bidirectional LSTM?"
      ],
      "metadata": {
        "id": "9B1Aw1S3t4VS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bidirectional LSTM Output Shape\n",
        "# ------------------- write your code here -------------------------------\n",
        "bidirectional_lstm = nn.LSTM(EMB_DIM, HIDDEN_DIM, batch_first=True, bidirectional=True)\n",
        "bidirectional_lstm_output, (bidirectional_lstm_hidden, bidirectional_lstm_cell) = bidirectional_lstm(embedded_batch)\n",
        "print(f\"'Output' tensor shape from a bidirectional LSTM: {bidirectional_lstm_output.shape}\")\n",
        "#-------------------------------------------------------------------------"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T17:48:51.968585Z",
          "iopub.execute_input": "2025-10-24T17:48:51.968881Z",
          "iopub.status.idle": "2025-10-24T17:48:52.117279Z",
          "shell.execute_reply.started": "2025-10-24T17:48:51.968860Z",
          "shell.execute_reply": "2025-10-24T17:48:52.116302Z"
        },
        "id": "DsDrU39At4VS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48c3dbfb-9485-4cc7-85e0-790f8c411ec0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Output' tensor shape from a bidirectional LSTM: torch.Size([64, 50, 512])\n"
          ]
        }
      ],
      "execution_count": 29
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q8. What is the 'hidden' state shape from a bidirectional LSTM?"
      ],
      "metadata": {
        "id": "3HoQngbQt4VS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bidirectional LSTM Hidden Shape\n",
        "# ------------------- write your code here -------------------------------\n",
        "print(f\"'Hidden' state shape from a bidirectional LSTM: {bidirectional_lstm_hidden.shape}\")\n",
        "#-------------------------------------------------------------------------"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T17:53:18.625946Z",
          "iopub.execute_input": "2025-10-24T17:53:18.626297Z",
          "iopub.status.idle": "2025-10-24T17:53:18.632909Z",
          "shell.execute_reply.started": "2025-10-24T17:53:18.626276Z",
          "shell.execute_reply": "2025-10-24T17:53:18.631609Z"
        },
        "id": "dRX_2qu3t4VS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3cbedbd-851c-4180-9310-954cb1666a2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Hidden' state shape from a bidirectional LSTM: torch.Size([2, 64, 256])\n"
          ]
        }
      ],
      "execution_count": 30
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q9. Create 3 sequential models using the (Simple & Bidirectional)LSTM and Stacked GRU (2 layers)For all models, follow this(Embedding layer ‚Üí [LSTM / BiLSTM / Stacked GRU] ‚Üí Linear layer) architecture. What will be the training parameters in all 3 cases?(LSTM, BiLSTM, Stacked GRU)"
      ],
      "metadata": {
        "id": "wDxAlIkvt4VS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T18:18:21.974713Z",
          "iopub.execute_input": "2025-10-24T18:18:21.975047Z",
          "iopub.status.idle": "2025-10-24T18:18:21.980302Z",
          "shell.execute_reply.started": "2025-10-24T18:18:21.975020Z",
          "shell.execute_reply": "2025-10-24T18:18:21.979147Z"
        },
        "id": "Y1xjyI8et4VT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- write your code here -------------------------------\n",
        "# Simple LSTM\n",
        "simple_lstm_model = SimpleLSTM(VOCAB_SIZE, EMB_DIM, HIDDEN_DIM, OUTPUT_DIM, PAD_IDX)\n",
        "simple_lstm_params = sum(p.numel() for p in simple_lstm_model.parameters() if p.requires_grad)\n",
        "print(f\"Number of training parameters for Simple LSTM: {simple_lstm_params}\")\n",
        "\n",
        "# Bidirectional LSTM\n",
        "bilstm_model = BiLSTM(VOCAB_SIZE, EMB_DIM, HIDDEN_DIM, OUTPUT_DIM, PAD_IDX)\n",
        "bilstm_params = sum(p.numel() for p in bilstm_model.parameters() if p.requires_grad)\n",
        "print(f\"Number of training parameters for Bidirectional LSTM: {bilstm_params}\")\n",
        "\n",
        "# Stacked GRU\n",
        "stacked_gru_model = StackedGRU(VOCAB_SIZE, EMB_DIM, HIDDEN_DIM, OUTPUT_DIM, PAD_IDX)\n",
        "stacked_gru_params = sum(p.numel() for p in stacked_gru_model.parameters() if p.requires_grad)\n",
        "print(f\"Number of training parameters for Stacked GRU: {stacked_gru_params}\")\n",
        "#-------------------------------------------------------------------------"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T18:21:28.488340Z",
          "iopub.execute_input": "2025-10-24T18:21:28.488678Z",
          "iopub.status.idle": "2025-10-24T18:21:28.530039Z",
          "shell.execute_reply.started": "2025-10-24T18:21:28.488657Z",
          "shell.execute_reply": "2025-10-24T18:21:28.529049Z"
        },
        "id": "jN6AU6wjt4VT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9d67f96-59a7-47fa-a13e-b5a15e77f401"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training parameters for Simple LSTM: 940877\n",
            "Number of training parameters for Bidirectional LSTM: 1308749\n",
            "Number of training parameters for Stacked GRU: 1243981\n"
          ]
        }
      ],
      "execution_count": 32
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q10. If you experimented with both LSTM and GRU models using the same hyperparameters, which one achieved a better peak Macro F1-score in your W&B logs?"
      ],
      "metadata": {
        "id": "cM_78-Est4VT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# LSTM vs GRU (Multi-label)\n",
        "# ============================\n",
        "import re, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using:\", DEVICE)\n",
        "\n",
        "# ---------- Load ----------\n",
        "train_path = \"/content/train (2).csv\"\n",
        "df = pd.read_csv(train_path)\n",
        "\n",
        "text_col = \"text\"\n",
        "# Multi-label columns present in your dataset\n",
        "label_cols = [\"anger\", \"fear\", \"joy\", \"sadness\", \"surprise\"]\n",
        "assert all(c in df.columns for c in label_cols), \"Some multi-label columns missing.\"\n",
        "\n",
        "# Clean text\n",
        "df[text_col] = df[text_col].fillna(\"\").astype(str)\n",
        "\n",
        "# ---------- Tokenizer + Vocab ----------\n",
        "def tokenize(x):\n",
        "    return re.findall(r\"[A-Za-z0-9']+\", x.lower())\n",
        "\n",
        "counter = Counter()\n",
        "for t in df[text_col]:\n",
        "    counter.update(tokenize(t))\n",
        "\n",
        "# Use min_freq=3 to stabilize vocab\n",
        "counter = Counter({w:c for w,c in counter.items() if c >= 3})\n",
        "\n",
        "PAD, UNK = \"<pad>\", \"<unk>\"\n",
        "itos = [PAD, UNK] + sorted(counter.keys())\n",
        "stoi = {w:i for i,w in enumerate(itos)}\n",
        "PAD_IDX, UNK_IDX = 0, 1\n",
        "VOCAB_SIZE = len(itos)\n",
        "\n",
        "print(f\"Vocab size: {VOCAB_SIZE} | PAD={PAD_IDX} UNK={UNK_IDX}\")\n",
        "\n",
        "def encode(tokens, max_len=60):\n",
        "    ids = [stoi.get(t, UNK_IDX) for t in tokens]\n",
        "    ids = ids[:max_len] + [PAD_IDX] * max(0, max_len - len(ids))\n",
        "    return ids\n",
        "\n",
        "# ---------- Dataset ----------\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels.astype(\"float32\")\n",
        "    def __len__(self): return len(self.texts)\n",
        "    def __getitem__(self, i):\n",
        "        x = torch.tensor(encode(tokenize(self.texts[i])), dtype=torch.long)\n",
        "        y = torch.tensor(self.labels[i], dtype=torch.float32)\n",
        "        return x, y\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    df[text_col].values,\n",
        "    df[label_cols].values,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(TextDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
        "val_loader   = DataLoader(TextDataset(X_val,   y_val),   batch_size=64, shuffle=False)\n",
        "\n",
        "# ---------- Models ----------\n",
        "EMB_DIM = 128\n",
        "HIDDEN  = 128\n",
        "EPOCHS  = 5\n",
        "LR      = 2e-3\n",
        "NUM_CLASSES = len(label_cols)\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.emb  = nn.Embedding(VOCAB_SIZE, EMB_DIM, padding_idx=PAD_IDX)\n",
        "        self.rnn  = nn.LSTM(EMB_DIM, HIDDEN, batch_first=True)\n",
        "        self.fc   = nn.Linear(HIDDEN, NUM_CLASSES)\n",
        "    def forward(self, x):\n",
        "        e = self.emb(x)\n",
        "        out, (h, c) = self.rnn(e)   # h: (1, B, H)\n",
        "        return self.fc(h[-1])       # (B, C)\n",
        "\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.emb  = nn.Embedding(VOCAB_SIZE, EMB_DIM, padding_idx=PAD_IDX)\n",
        "        self.rnn  = nn.GRU(EMB_DIM, HIDDEN, batch_first=True)\n",
        "        self.fc   = nn.Linear(HIDDEN, NUM_CLASSES)\n",
        "    def forward(self, x):\n",
        "        e = self.emb(x)\n",
        "        out, h = self.rnn(e)        # h: (1, B, H)\n",
        "        return self.fc(h[-1])       # (B, C)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_macro_f1(model, loader):\n",
        "    model.eval()\n",
        "    preds, trues = [], []\n",
        "    for x, y in loader:\n",
        "        x = x.to(DEVICE)\n",
        "        logits = model(x)\n",
        "        probs  = torch.sigmoid(logits)\n",
        "        p = (probs > 0.5).int().cpu().numpy()\n",
        "        preds.append(p)\n",
        "        trues.append(y.numpy())\n",
        "    y_true = np.vstack(trues)\n",
        "    y_pred = np.vstack(preds)\n",
        "    return f1_score(y_true, y_pred, average=\"macro\")\n",
        "\n",
        "def train_and_eval(model):\n",
        "    model = model.to(DEVICE)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LR)\n",
        "    best_f1 = 0.0\n",
        "    for ep in range(1, EPOCHS+1):\n",
        "        model.train()\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "            opt.zero_grad()\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "        f1 = eval_macro_f1(model, val_loader)\n",
        "        best_f1 = max(best_f1, f1)\n",
        "        print(f\"Epoch {ep}: val Macro F1 = {f1:.4f}\")\n",
        "    return best_f1\n",
        "\n",
        "print(\"\\n--- Training LSTM (multi-label) ---\")\n",
        "best_lstm = train_and_eval(LSTMModel())\n",
        "\n",
        "print(\"\\n--- Training GRU (multi-label) ---\")\n",
        "best_gru = train_and_eval(GRUModel())\n",
        "\n",
        "print(\"\\n=== Comparison ===\")\n",
        "print(f\"Best LSTM Macro F1: {best_lstm:.4f}\")\n",
        "print(f\"Best GRU  Macro F1: {best_gru:.4f}\")\n",
        "\n",
        "if best_lstm > best_gru:\n",
        "    print(\"\\n‚úÖ LSTM achieved the better peak Macro F1-score.\")\n",
        "elif best_gru > best_lstm:\n",
        "    print(\"\\n‚úÖ GRU achieved the better peak Macro F1-score.\")\n",
        "else:\n",
        "    print(\"\\n‚öñÔ∏è Both performed equally.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "7JuXKBDLt4VT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d589096-d010-4915-f2b7-1045d2fb38d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using: cpu\n",
            "Vocab size: 3246 | PAD=0 UNK=1\n",
            "\n",
            "--- Training LSTM (multi-label) ---\n",
            "Epoch 1: val Macro F1 = 0.1457\n",
            "Epoch 2: val Macro F1 = 0.1490\n",
            "Epoch 3: val Macro F1 = 0.1535\n",
            "Epoch 4: val Macro F1 = 0.1540\n",
            "Epoch 5: val Macro F1 = 0.1587\n",
            "\n",
            "--- Training GRU (multi-label) ---\n",
            "Epoch 1: val Macro F1 = 0.1459\n",
            "Epoch 2: val Macro F1 = 0.1510\n",
            "Epoch 3: val Macro F1 = 0.2187\n",
            "Epoch 4: val Macro F1 = 0.2664\n",
            "Epoch 5: val Macro F1 = 0.2693\n",
            "\n",
            "=== Comparison ===\n",
            "Best LSTM Macro F1: 0.1587\n",
            "Best GRU  Macro F1: 0.2693\n",
            "\n",
            "‚úÖ GRU achieved the better peak Macro F1-score.\n"
          ]
        }
      ],
      "execution_count": 39
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()\n",
        "df.columns\n"
      ],
      "metadata": {
        "id": "OZ2RC9sSoV7g",
        "outputId": "46539348-8ea2-4477-cd06-0314f9851b16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', 'text', 'anger', 'fear', 'joy', 'sadness', 'surprise',\n",
              "       'emotions'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q11. Compare the total training time for your best sequential model against the simple averaging model from Milestone 3. How much longer (in minutes or percentage) did the more complex model (LSTM and GRU) take to train for the same number of epochs?"
      ],
      "metadata": {
        "id": "rKXe8Bcht4VT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# 1) Simple Averaging Baseline Model (Milestone 3)\n",
        "# ===========================================\n",
        "class SimpleAvgModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(VOCAB_SIZE, EMB_DIM, padding_idx=PAD_IDX)\n",
        "        self.fc = nn.Linear(EMB_DIM, NUM_CLASSES)\n",
        "\n",
        "    def forward(self, x):\n",
        "        e = self.emb(x)                             # (B, L, E)\n",
        "        mask = (x != PAD_IDX).unsqueeze(-1)         # (B, L, 1)\n",
        "        summed = (e * mask).sum(dim=1)              # (B, E)\n",
        "        denom = mask.sum(dim=1).clamp(min=1)        # (B, 1)\n",
        "        avg = summed / denom                        # (B, E)\n",
        "        return self.fc(avg)                         # (B, C)\n",
        "\n",
        "\n",
        "def train_avg():\n",
        "    model = SimpleAvgModel().to(DEVICE)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LR)\n",
        "    start = time.time()\n",
        "\n",
        "    for ep in range(1, EPOCHS+1):\n",
        "        model.train()\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "            opt.zero_grad()\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "    end = time.time()\n",
        "    return end - start\n",
        "\n",
        "\n",
        "# Run baseline model\n",
        "print(\"\\n--- Timing Simple Averaging Model ---\")\n",
        "baseline_time = train_avg()\n",
        "print(f\"Baseline Training Time: {baseline_time:.2f} sec\")\n",
        "\n",
        "\n",
        "# ===========================================\n",
        "# 2) Timing your best sequential model\n",
        "# Replace GRUModel() with LSTMModel() if LSTM performed better\n",
        "# ===========================================\n",
        "def train_seq():\n",
        "    model = GRUModel().to(DEVICE)   # <-- Change to LSTMModel() if LSTM > GRU\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=LR)\n",
        "    start = time.time()\n",
        "\n",
        "    for ep in range(1, EPOCHS+1):\n",
        "        model.train()\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "            opt.zero_grad()\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "    end = time.time()\n",
        "    return end - start\n",
        "\n",
        "\n",
        "print(\"\\n--- Timing Best Sequential Model ---\")\n",
        "seq_time = train_seq()\n",
        "print(f\"Sequential Model Training Time: {seq_time:.2f} sec\")\n",
        "\n",
        "\n",
        "# ===========================================\n",
        "# 3) Compare Times\n",
        "# ===========================================\n",
        "extra_minutes = (seq_time - baseline_time) / 60\n",
        "percent_more = (seq_time - baseline_time) / baseline_time * 100\n",
        "\n",
        "print(\"\\n===== Time Comparison Result =====\")\n",
        "print(f\"The sequential model took {extra_minutes:.2f} more minutes.\")\n",
        "print(f\"Which is {percent_more:.1f}% longer than the simple averaging model.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "DMt4dVRbt4VT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9103c5ed-a2af-4365-fcbb-fd74586a3afb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Timing Simple Averaging Model ---\n",
            "Baseline Training Time: 6.94 sec\n",
            "\n",
            "--- Timing Best Sequential Model ---\n",
            "Sequential Model Training Time: 36.26 sec\n",
            "\n",
            "===== Time Comparison Result =====\n",
            "The sequential model took 0.49 more minutes.\n",
            "Which is 422.7% longer than the simple averaging model.\n"
          ]
        }
      ],
      "execution_count": 40
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q12. If you experimented with both LSTM and GRU models using the same hyperparameters, which one achieved a better peak Macro F1-score in your W&B logs?"
      ],
      "metadata": {
        "id": "vmHxpiLst4VT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "ExtGs52Ct4Vc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q13 Based on your experiments, what was the most impactful hyperparameter you tuned for your sequential model (e.g., learning rate, hidden size, number of layers, dropout rate)?"
      ],
      "metadata": {
        "id": "TA7mcOOKt4Vc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iog5kgTt65FI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}