# -*- coding: utf-8 -*-
"""preprocessing_roberta

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/ashish4129/preprocessing-roberta.fdab98ed-b8f9-4ebc-a101-37343142a405.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20251129/auto/storage/goog4_request%26X-Goog-Date%3D20251129T043529Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D6ec90a0e565ed8b346687aa78dbe753a456c442115203cb0d3cfb799c2f840ee5f1627fa5a20617846fb889b4cbbf07e54c752430749d6123f4b482848baec250207b8eec198d6cc547b8ed87ed3e2b66fcb873e6da2ef4822857694b9c4e9452b4224b6b7870303ca3f225b5b68b544ea675d0a459181717037c856015503e93edd9f1c06d0398991dd0ac6e06b342f538585895dbafabdc016339ea960dc9b1c172256294ca3903625fedc6b47f6f5241f280a288812efb1cd133788f4bb5e047804d916c8776602df2fbc02219c33c8769680522d1a7cacb6dbd6be54ccf2f835e21e5a40d7f05de1698f258ad7bb7debd3de5a6b6b4aa2c6b09e8ffdd36e
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

2025_sep_dl_gen_ai_project_path = kagglehub.competition_download('2025-sep-dl-gen-ai-project')

print('Data source import complete.')

!pip install datasets transformers nltk --quiet

import pandas as pd
import string
import nltk
from nltk.corpus import stopwords
from collections import Counter
from sklearn.model_selection import train_test_split
from datasets import Dataset
from transformers import AutoTokenizer

nltk.download('stopwords')

train_df = pd.read_csv("/kaggle/input/2025-sep-dl-gen-ai-project/train.csv")
test_df  = pd.read_csv("/kaggle/input/2025-sep-dl-gen-ai-project/test.csv")

print("Train shape:", train_df.shape)
print("Test shape:", test_df.shape)
print(train_df.head())

train_df['clean_text'] = train_df['text'].str.lower()

before_count = train_df['clean_text'].str.len().sum()

translator = str.maketrans('', '', string.punctuation)
train_df['clean_text'] = train_df['clean_text'].apply(lambda x: x.translate(translator))

after_count = train_df['clean_text'].str.len().sum()

reduction_percent = ((before_count - after_count) / before_count) * 100
print(f"Percentage reduction in character count: {reduction_percent:.2f}%")

words = []
for text in train_df['clean_text']:
    words.extend(text.split())

unique_words = set(words)
print("Total unique words:", len(unique_words))

stop_words = set(stopwords.words('english'))
common_stopwords = unique_words.intersection(stop_words)
stopword_percent = (len(common_stopwords) / len(unique_words)) * 100
print(f"Percentage of unique words that are stop words: {stopword_percent:.2f}%")

filtered_words = [w for w in words if w not in stop_words]
word_freq = Counter(filtered_words)
most_common_words = word_freq.most_common(10)

print("\nTop 10 most frequent non-stop words:")
for i, (word, count) in enumerate(most_common_words, 1):
    print(f"{i}. {word} - {count}")

fifth_most_freq_word = most_common_words[4][0]
print(f"\n5th most frequent word (excluding stop words): '{fifth_most_freq_word}'")

labels = ['anger', 'fear', 'joy', 'sadness', 'surprise']

print("\nInitial rows:", train_df.shape[0])
print("Missing text rows in train:", train_df['text'].isnull().sum())

train_df = train_df.dropna(subset=['text']).reset_index(drop=True)

print("Rows after dropping null-text:", train_df.shape[0])

for l in labels:
    train_df[l] = train_df[l].astype(int)

if 'emotions' in train_df.columns:
    train_df = train_df.drop(columns=['emotions'])

print("\nLabel counts (whole train):")
print(train_df[labels].sum())

n_labels = train_df[labels].sum(axis=1)

print("\nDistribution of #labels per sample (whole train):")
print(n_labels.value_counts().sort_index())

train_data, val_data = train_test_split(
    train_df,
    test_size=0.10,
    random_state=42,
    shuffle=True
)

print(f"\nTrain rows: {len(train_data)}    |    Val rows: {len(val_data)}")
print("\nLabel counts (train split):")
print(train_data[labels].sum())
print("\nLabel counts (val split):")
print(val_data[labels].sum())

train_dataset = Dataset.from_pandas(train_data.reset_index(drop=True))
val_dataset   = Dataset.from_pandas(val_data.reset_index(drop=True))

print("\nExample training sample (dict):")
print(train_dataset[0])

print("\nTest missing text rows:", test_df['text'].isnull().sum())

MODEL_NAME = "roberta-base"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

def tokenize_function(example):
    return tokenizer(
        example["text"],
        truncation=True,
        padding="max_length",
        max_length=256
    )

train_tokenized = train_dataset.map(tokenize_function, batched=True)
val_tokenized   = val_dataset.map(tokenize_function, batched=True)

label_cols = ['anger', 'fear', 'joy', 'sadness', 'surprise']

for col in label_cols:
    train_tokenized = train_tokenized.rename_column(col, f"labels_{col}")
    val_tokenized   = val_tokenized.rename_column(col,   f"labels_{col}")

train_tokenized.set_format(
    "torch",
    columns=["input_ids", "attention_mask"] + [f"labels_{l}" for l in label_cols]
)

val_tokenized.set_format(
    "torch",
    columns=["input_ids", "attention_mask"] + [f"labels_{l}" for l in label_cols]
)

print("\nTokenization complete!")
print("Train tokenized sample:")
print(train_tokenized[0])