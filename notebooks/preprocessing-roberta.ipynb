{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":115439,"databundleVersionId":13800781,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets transformers nltk --quiet\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T04:32:09.526920Z","iopub.execute_input":"2025-11-29T04:32:09.527288Z","iopub.status.idle":"2025-11-29T04:32:19.228143Z","shell.execute_reply.started":"2025-11-29T04:32:09.527261Z","shell.execute_reply":"2025-11-29T04:32:19.227130Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T04:32:19.229610Z","iopub.execute_input":"2025-11-29T04:32:19.229953Z","iopub.status.idle":"2025-11-29T04:32:34.513028Z","shell.execute_reply.started":"2025-11-29T04:32:19.229916Z","shell.execute_reply":"2025-11-29T04:32:34.511908Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"nltk.download('stopwords')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T04:32:38.658500Z","iopub.execute_input":"2025-11-29T04:32:38.659134Z","iopub.status.idle":"2025-11-29T04:32:38.731308Z","shell.execute_reply.started":"2025-11-29T04:32:38.659108Z","shell.execute_reply":"2025-11-29T04:32:38.730447Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/2025-sep-dl-gen-ai-project/train.csv\")\ntest_df  = pd.read_csv(\"/kaggle/input/2025-sep-dl-gen-ai-project/test.csv\")\n\nprint(\"Train shape:\", train_df.shape)\nprint(\"Test shape:\", test_df.shape)\nprint(train_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T04:32:47.132387Z","iopub.execute_input":"2025-11-29T04:32:47.132710Z","iopub.status.idle":"2025-11-29T04:32:47.186237Z","shell.execute_reply.started":"2025-11-29T04:32:47.132686Z","shell.execute_reply":"2025-11-29T04:32:47.185269Z"}},"outputs":[{"name":"stdout","text":"Train shape: (6827, 8)\nTest shape: (1707, 2)\n   id                                               text  anger  fear  joy  \\\n0   0  the dentist that did the work apparently did a...      1     0    0   \n1   1  i'm gonna absolutely ~~suck~~ be terrible duri...      0     1    0   \n2   2  bridge: so leave me drowning calling houston, ...      0     1    0   \n3   3  after that mess i went to see my now ex-girlfr...      1     1    0   \n4   4  as he stumbled i ran off, afraid it might some...      0     1    0   \n\n   sadness  surprise                    emotions  \n0        1         0         ['anger' 'sadness']  \n1        1         0          ['fear' 'sadness']  \n2        1         0          ['fear' 'sadness']  \n3        1         0  ['anger' 'fear' 'sadness']  \n4        0         0                    ['fear']  \n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"train_df['clean_text'] = train_df['text'].str.lower()\n\nbefore_count = train_df['clean_text'].str.len().sum()\n\ntranslator = str.maketrans('', '', string.punctuation)\ntrain_df['clean_text'] = train_df['clean_text'].apply(lambda x: x.translate(translator))\n\nafter_count = train_df['clean_text'].str.len().sum()\n\nreduction_percent = ((before_count - after_count) / before_count) * 100\nprint(f\"Percentage reduction in character count: {reduction_percent:.2f}%\")\n\nwords = []\nfor text in train_df['clean_text']:\n    words.extend(text.split())\n\nunique_words = set(words)\nprint(\"Total unique words:\", len(unique_words))\n\nstop_words = set(stopwords.words('english'))\ncommon_stopwords = unique_words.intersection(stop_words)\nstopword_percent = (len(common_stopwords) / len(unique_words)) * 100\nprint(f\"Percentage of unique words that are stop words: {stopword_percent:.2f}%\")\n\nfiltered_words = [w for w in words if w not in stop_words]\nword_freq = Counter(filtered_words)\nmost_common_words = word_freq.most_common(10)\n\nprint(\"\\nTop 10 most frequent non-stop words:\")\nfor i, (word, count) in enumerate(most_common_words, 1):\n    print(f\"{i}. {word} - {count}\")\n\nfifth_most_freq_word = most_common_words[4][0]\nprint(f\"\\n5th most frequent word (excluding stop words): '{fifth_most_freq_word}'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T04:32:55.422322Z","iopub.execute_input":"2025-11-29T04:32:55.422632Z","iopub.status.idle":"2025-11-29T04:32:55.495850Z","shell.execute_reply.started":"2025-11-29T04:32:55.422610Z","shell.execute_reply":"2025-11-29T04:32:55.495087Z"}},"outputs":[{"name":"stdout","text":"Percentage reduction in character count: 3.26%\nTotal unique words: 8365\nPercentage of unique words that are stop words: 1.54%\n\nTop 10 most frequent non-stop words:\n1. head - 539\n2. eyes - 438\n3. like - 394\n4. back - 365\n5. heart - 334\n6. one - 323\n7. face - 293\n8. get - 291\n9. time - 271\n10. still - 271\n\n5th most frequent word (excluding stop words): 'heart'\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"labels = ['anger', 'fear', 'joy', 'sadness', 'surprise']\n\nprint(\"\\nInitial rows:\", train_df.shape[0])\nprint(\"Missing text rows in train:\", train_df['text'].isnull().sum())\n\ntrain_df = train_df.dropna(subset=['text']).reset_index(drop=True)\n\nprint(\"Rows after dropping null-text:\", train_df.shape[0])\n\nfor l in labels:\n    train_df[l] = train_df[l].astype(int)\n\nif 'emotions' in train_df.columns:\n    train_df = train_df.drop(columns=['emotions'])\n\nprint(\"\\nLabel counts (whole train):\")\nprint(train_df[labels].sum())\n\nn_labels = train_df[labels].sum(axis=1)\n\nprint(\"\\nDistribution of #labels per sample (whole train):\")\nprint(n_labels.value_counts().sort_index())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T04:33:07.117076Z","iopub.execute_input":"2025-11-29T04:33:07.117892Z","iopub.status.idle":"2025-11-29T04:33:07.150837Z","shell.execute_reply.started":"2025-11-29T04:33:07.117861Z","shell.execute_reply":"2025-11-29T04:33:07.149466Z"}},"outputs":[{"name":"stdout","text":"\nInitial rows: 6827\nMissing text rows in train: 0\nRows after dropping null-text: 6827\n\nLabel counts (whole train):\nanger        808\nfear        3860\njoy         1660\nsadness     2171\nsurprise    1999\ndtype: int64\n\nDistribution of #labels per sample (whole train):\n0     676\n1    2743\n2    2587\n3     706\n4     112\n5       3\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"train_data, val_data = train_test_split(\n    train_df,\n    test_size=0.10,\n    random_state=42,\n    shuffle=True\n)\n\nprint(f\"\\nTrain rows: {len(train_data)}    |    Val rows: {len(val_data)}\")\nprint(\"\\nLabel counts (train split):\")\nprint(train_data[labels].sum())\nprint(\"\\nLabel counts (val split):\")\nprint(val_data[labels].sum())\n\ntrain_dataset = Dataset.from_pandas(train_data.reset_index(drop=True))\nval_dataset   = Dataset.from_pandas(val_data.reset_index(drop=True))\n\nprint(\"\\nExample training sample (dict):\")\nprint(train_dataset[0])\n\nprint(\"\\nTest missing text rows:\", test_df['text'].isnull().sum())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T04:33:16.945439Z","iopub.execute_input":"2025-11-29T04:33:16.945824Z","iopub.status.idle":"2025-11-29T04:33:16.996491Z","shell.execute_reply.started":"2025-11-29T04:33:16.945798Z","shell.execute_reply":"2025-11-29T04:33:16.995581Z"}},"outputs":[{"name":"stdout","text":"\nTrain rows: 6144    |    Val rows: 683\n\nLabel counts (train split):\nanger        728\nfear        3468\njoy         1497\nsadness     1961\nsurprise    1798\ndtype: int64\n\nLabel counts (val split):\nanger        80\nfear        392\njoy         163\nsadness     210\nsurprise    201\ndtype: int64\n\nExample training sample (dict):\n{'id': 2276, 'text': 'it leaves both my hands free to be able to type.', 'anger': 0, 'fear': 0, 'joy': 1, 'sadness': 0, 'surprise': 0, 'clean_text': 'it leaves both my hands free to be able to type'}\n\nTest missing text rows: 0\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"MODEL_NAME = \"roberta-base\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\ndef tokenize_function(example):\n    return tokenizer(\n        example[\"text\"],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=256\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T04:33:26.607008Z","iopub.execute_input":"2025-11-29T04:33:26.607321Z","iopub.status.idle":"2025-11-29T04:33:28.739655Z","shell.execute_reply.started":"2025-11-29T04:33:26.607292Z","shell.execute_reply":"2025-11-29T04:33:28.738718Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86ea84e725da41709b715ad36d8c59d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de4a5c7a16a4461492a0c82b0c5c16bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad253eb64b564c22b7f2d7294054a37f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cc7183d4298407dbf620f588d2d090c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6a0e8d4a547477ba3b9dde606c914d6"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"train_tokenized = train_dataset.map(tokenize_function, batched=True)\nval_tokenized   = val_dataset.map(tokenize_function, batched=True)\n\nlabel_cols = ['anger', 'fear', 'joy', 'sadness', 'surprise']\n\nfor col in label_cols:\n    train_tokenized = train_tokenized.rename_column(col, f\"labels_{col}\")\n    val_tokenized   = val_tokenized.rename_column(col,   f\"labels_{col}\")\n\ntrain_tokenized.set_format(\n    \"torch\",\n    columns=[\"input_ids\", \"attention_mask\"] + [f\"labels_{l}\" for l in label_cols]\n)\n\nval_tokenized.set_format(\n    \"torch\",\n    columns=[\"input_ids\", \"attention_mask\"] + [f\"labels_{l}\" for l in label_cols]\n)\n\nprint(\"\\nTokenization complete!\")\nprint(\"Train tokenized sample:\")\nprint(train_tokenized[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T04:33:35.473153Z","iopub.execute_input":"2025-11-29T04:33:35.473499Z","iopub.status.idle":"2025-11-29T04:33:36.696824Z","shell.execute_reply.started":"2025-11-29T04:33:35.473474Z","shell.execute_reply":"2025-11-29T04:33:36.695796Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6144 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c32172451454494abc125e14744d737"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/683 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b14c4d2874984dac91f340170ccfaaac"}},"metadata":{}},{"name":"stdout","text":"\n✅ Tokenization complete!\nTrain tokenized sample:\n{'labels_anger': tensor(0), 'labels_fear': tensor(0), 'labels_joy': tensor(1), 'labels_sadness': tensor(0), 'labels_surprise': tensor(0), 'input_ids': tensor([   0,  405, 3607,  258,  127, 1420,  481,    7,   28,  441,    7, 1907,\n           4,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n           1,    1,    1,    1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\n","output_type":"stream"}],"execution_count":9}]}